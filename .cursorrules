# Instructions

During your interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

You should also use the `.cursorrules` file as a Scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the Scratchpad, clear old different task if necessary, first explain the task, and plan the steps you need to take to complete the task. You can use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the Scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification

The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot

screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM

response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a python venv in ./venv. Use it.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use 'gpt-4o' as the model name for OpenAI's GPT-4 with vision capabilities
- Stack size errors in Solana programs may require specific version combinations of solana-program and anchor
- When encountering stack size errors in Solana programs:
  1. Try different versions of solana-program
  2. Configure stack size in .cargo/config.toml
  3. Optimize profile settings in Cargo.toml
  4. Consider splitting large functions or reducing stack variable sizes
- When working with Solana program dependencies:
  1. Ensure version compatibility between anchor-lang and solana-program
  2. Use default-features = false to minimize included features
  3. Consider using older, more stable versions for complex programs
  4. Watch for transitive dependency conflicts
- When dealing with Solana program stack size errors:
  1. Check for recursive or deeply nested functions
  2. Minimize string and vector usage in program state
  3. Consider using fixed-size arrays where possible
  4. Move complex computations off-chain when feasible
- When building Solana programs with Anchor:
  1. Use compatible version combinations:
     - Solana toolchain 1.16.0 with Anchor 0.28.0
     - solana-program 1.16.0 with anchor-lang 0.28.0
     - anchor-spl 0.28.0 for token integration
  2. Configure stack size and build optimization:
     ```toml
     # .cargo/config.toml
     [build]
     rustflags = [
         "-C", "link-arg=-zstack-size=65536",
         "-C", "target-cpu=native",
     ]

     # Cargo.toml
     [profile.release]
     overflow-checks = true
     lto = "fat"
     codegen-units = 1
     opt-level = "z"
     panic = "abort"
     strip = true
     debug = false
     incremental = false
     ```
  3. Handle references properly in Anchor program code:
     - Use `&Account<T>` for read-only access
     - Use `&mut Account<T>` for mutable access
     - Pay attention to lifetime parameters in struct definitions

# Scratchpad

## Project Analysis: Solana Node Editor

### Overview
This is a visual node-based development environment for Solana blockchain applications, designed to simplify blockchain development through a drag-and-drop interface.

### Core Components

1. Frontend (React-based)
[X] Visual node editor interface
[X] Component-based architecture
[X] React hooks for state management
[X] API integration services
[X] Type definitions and prop types

2. Backend (Node.js + Solana)
[X] Custom DAO program implementation
[X] Solana program management
[X] Testing infrastructure
[X] Script utilities
[X] Contract deployment tools

3. Development Tools
[X] Screenshot verification system
[X] Multi-provider LLM integration
[X] Web scraping capabilities
[X] Search functionality
[X] Python-based utilities

### Key Features
[X] Visual workflow builder
[X] Solana program lifecycle management
[X] AI-powered code generation
[X] Secure key management
[X] Docker environment
[X] Real-time blockchain simulation

### Node Types
1. DeFi & Liquidity
   - AMM Pool
   - Liquidity Locker
   - Flash Loan
   - Leverage Engine
   - Oracle Integration

2. NFT & Digital Assets
   - Dynamic NFT
   - Royalty Management
   - Burn/Melt Mechanics
   - Rarity Engine
   - Bundle Creation

3. DAO & Governance
   - Custom DAO Implementation
   - Proposal Management
   - Treasury Control
   - Voting Systems
   - Compliance Tools

### Current Focus
[X] Custom DAO Program Development
- Simplified governance structure
- Token integration
- Proposal management
- Voting mechanics

### Next Steps
[ ] Review custom DAO implementation
[ ] Analyze frontend-backend integration
[ ] Examine node system architecture
[ ] Evaluate testing coverage
[ ] Document security measures

## Current Task: Codebase Analysis

### Goals
[X] Understand project structure and key components
[X] Identify main functionality
[X] Note key dependencies and technologies
[ ] Document potential areas for improvement

### Project Overview

1. Core Technologies
- Solana blockchain platform
- React/TypeScript for frontend development
- Anchor framework for Solana program development
- Node-based visual programming interface
- Python tooling for development support

2. Key Components

Frontend:
- Visual node editor for blockchain operations
- Drag-and-drop interface for creating workflows
- Real-time transaction preview and validation
- Component library for different blockchain operations
- State management for complex workflows

Backend/Tools:
- Python-based development tools:
  - Screenshot verification system
  - Multi-provider LLM integration (OpenAI, Anthropic, etc.)
  - Web scraping and search capabilities
  - Development utilities

Blockchain:
- Solana programs for:
  - Token management
  - DAO governance
  - Metadata handling
- Anchor framework integration
- Transaction building and execution

3. Main Features
- Visual blockchain programming interface
- Token creation and management
- DAO setup and governance
- Metadata handling for tokens
- Development tools integration
- Multi-provider LLM support
- Web scraping and verification tools

4. Development Tools
- Screenshot verification workflow
- LLM integration with multiple providers
- Web scraping capabilities
- Search engine integration
- Python-based utility scripts

5. Project Structure
- /src - Main application code
  - /components - React components
  - /services - Blockchain services
  - /utils - Utility functions
- /tools - Python development tools
- /programs - Solana programs
- /tests - Test suites

6. Notable Characteristics
- Focus on developer experience with comprehensive tooling
- Visual programming approach to blockchain operations
- Strong emphasis on verification and testing
- Multiple AI provider integration
- Modular architecture for extensibility

### Areas for Further Investigation
[ ] Error handling patterns across components
[ ] Transaction building and validation flow
[ ] State management architecture
[ ] Testing coverage and methodology
[ ] Documentation completeness

## Current Task: Token Creation with Metadata Analysis

### Component Overview
[X] Token Node UI Component
- Input fields:
  - Name: "GameFi Token"
  - Symbol: "GAME"
  - Decimals: 9
  - Initial Supply: 101
  - Mint Authority (optional)
  - Freeze Authority (optional)

### Token Creation Flow with Metadata

1. Token Node Creation
[X] UI Component (TokenNode.js)
- Collects token parameters
- Validates input
- Manages creation state
- Handles error display

2. Token Creation Process
[X] Core Token Creation
```typescript
// Basic token creation
const mintAccount = Keypair.generate();
const decimals = 9;
const supply = 101;

// Create and initialize mint account
// ... (as shown in previous analysis)
```

3. Metadata Integration
[X] Token Metadata Program
```typescript
// Create metadata account
const metadataAddress = await getMetadataAddress(mintAccount.publicKey);
const createMetadataIx = createCreateMetadataAccountV3Instruction({
    metadata: metadataAddress,
    mint: mintAccount.publicKey,
    mintAuthority: payer.publicKey,
    payer: payer.publicKey,
    updateAuthority: payer.publicKey,
    data: {
        name: "GameFi Token",
        symbol: "GAME",
        uri: "", // Optional: URI for extended metadata
        sellerFeeBasisPoints: 0,
        creators: null,
        collection: null,
        uses: null
    }
});
```

### Key Files to Review
1. src/components/Nodes/TokenNode.js
- UI component
- State management
- Error handling

2. src/services/token.js
- Token creation logic
- Metadata integration
- Transaction building

3. src/utils/metadata.js
- Metadata account creation
- PDA derivation
- Program constants

### Potential Error Sources
[ ] Metadata Account Creation
- Invalid PDA derivation
- Insufficient account space
- Missing required signers

[ ] Transaction Building
- Incorrect instruction order
- Missing required accounts
- Transaction size limits

[ ] Account Validation
- Metadata program ID mismatch
- Authority validation
- Account ownership checks

### Next Steps
[ ] Review error handling in TokenNode.js
[ ] Check metadata account creation logic
[ ] Verify transaction building sequence
[ ] Test authority management
[ ] Add proper error messages

Would you like me to examine any specific part of this implementation?

## New Implementation Plan: Custom Anchor DAO

### Current Issues
1. SPL Governance program version schema error
2. Complex integration with multiple dependencies
3. Version compatibility issues

### Proposed Solution: Custom Anchor DAO Implementation

[X] Analysis Phase
- Reviewed current implementation
- Identified failure points
- Examined Anchor DAO alternative

[ ] Implementation Phase
1. Create Custom DAO Program
   - Basic DAO structure from anchor-dao-notes.md
   - Add token integration
   - Implement proposal and voting logic

2. Backend Integration
   - Replace SPL Governance calls with custom program
   - Update SolanaClient.js to use new DAO program
   - Add proper error handling

3. Frontend Updates
   - Modify DAONode.js to work with custom program
   - Update validation logic
   - Simplify state management

### Technical Details

1. DAO Account Structure:
```rust
#[account]
pub struct Dao {
    pub authority: Pubkey,
    pub community_token: Pubkey,
    pub proposal_count: u64,
    pub voting_threshold: u8,
    pub max_voting_time: i64,
    pub hold_up_time: i64,
}
```

2. Proposal Structure:
```rust
#[account]
pub struct Proposal {
    pub creator: Pubkey,
    pub description: String,
    pub vote_count: u64,
    pub start_time: i64,
    pub end_time: i64,
    pub executed: bool,
}
```

3. Key Functions:
   - initialize_dao
   - create_proposal
   - cast_vote
   - execute_proposal

### Migration Steps
1. [ ] Create new Anchor program
2. [ ] Deploy program to devnet/testnet
3. [ ] Update backend integration
4. [ ] Modify frontend components
5. [ ] Add comprehensive testing

### Benefits
1. Simplified architecture
2. Direct control over program logic
3. Easier to maintain and update
4. Better error handling
5. No version compatibility issues

### Risks and Mitigations
1. Risk: Custom implementation security
   - Mitigation: Thorough security review and testing
2. Risk: Migration complexity
   - Mitigation: Phased rollout with feature flags
3. Risk: User experience changes
   - Mitigation: Comprehensive documentation and UI guidance

## Current Task: Resolving Solana Program Stack Size Error

### Attempted Solutions
[X] Move profile settings to workspace Cargo.toml
[X] Update solana-program version (1.16.0 -> 1.14.0)
[X] Add stack size configuration in .cargo/config.toml
[X] Add resolver specification for edition 2021
[X] Try alternative version combinations:
    - anchor 0.27.0 + solana-program 1.14.17
    - anchor 0.26.0 + solana-program 1.14.18
    - anchor 0.25.0 + solana-program 1.10.29
    - anchor 0.24.2 + solana-program 1.9.29
    - anchor 0.24.2 + solana-program 1.9.9
[X] Refactor program structure:
    - Split large functions into smaller ones
    - Move configuration into separate struct
    - Optimize account space calculations
    - Use more efficient data types

### Current Issues
1. Stack size error persists across different version combinations
2. Dependency conflicts between anchor and solana-program versions
3. Issues with transitive dependencies (atty, termcolor, regex)
4. Core issue appears to be in the regex-automata crate

### New Approach: Core Issue Focus
1. Identify Core Problem
   - Stack size error originates from regex-automata crate
   - Error occurs during build process, not in our code
   - Issue likely in build dependencies

2. Proposed Solutions
   - Try using BPF linker directly
   - Investigate alternative build toolchain
   - Consider pre-built dependencies
   - Look for regex-automata workarounds

3. Investigation Steps
   [ ] Check if regex-automata is a direct dependency
   [ ] Identify which crate brings in regex-automata
   [ ] Look for alternative crates without regex dependency
   [ ] Consider building with different toolchain

### Next Steps
1. [ ] Investigate regex-automata dependency chain
2. [ ] Try building with cargo-build-bpf
3. [ ] Consider alternative build approaches
4. [ ] Document findings for future reference

### Benefits
1. Better understanding of dependency issues
2. More targeted solution approach
3. Potential workaround discovery
4. Knowledge base for similar issues

### Risks and Mitigations
1. Risk: Build toolchain complexity
   - Mitigation: Document build process
2. Risk: Dependency conflicts
   - Mitigation: Map dependency tree
3. Risk: Version lock-in
   - Mitigation: Plan upgrade path

## Current Task: RPC URL Configuration

### Goals
[X] Update RPC configuration across all components
[X] Ensure consistent network settings
[X] Verify connection stability
[X] Document configuration changes

### Current Configuration
1. Backend:
   - Network: devnet
   - RPC URL: https://api.devnet.solana.com
   - WS URL: wss://api.devnet.solana.com
   - Status: Connected and healthy

2. Frontend:
   - Network: devnet
   - RPC Endpoint: https://api.devnet.solana.com
   - Status: Configured

### Configuration Summary
1. [X] Updated backend .env
   - Set network to devnet
   - Configured RPC and WebSocket URLs
   - Set proper commitment level
   - Updated program IDs

2. [X] Updated frontend .env
   - Set network to devnet
   - Configured RPC endpoint
   - Updated program IDs
   - Set API URL

3. [X] Verified configuration
   - Backend health check successful
   - Environment variables properly loaded
   - Connection to devnet established

4. [X] Connection tested
   - Backend server responding
   - Health endpoint working
   - RPC connection verified

### Important Notes
1. Using Solana devnet for development
2. Backend and frontend properly configured
3. All necessary program IDs updated
4. Connection to devnet verified and stable

### Next Steps (if needed)
1. Monitor connection stability
2. Watch for any RPC rate limiting
3. Consider fallback RPC endpoints
4. Implement connection retry logic

## Current Task: NFT Implementation Analysis

### Core Components

1. NFT Architecture
[X] Three-tier system:
   - Frontend: React-based NFT node editor
   - Backend: Metaplex NFT service integration
   - Blockchain: Solana NFT program integration

2. NFT Structure
[X] Basic Components:
   - SPL Token with 0 decimals
   - Maximum supply of 1
   - Associated metadata account
   - Metaplex Token Metadata program integration

3. Implementation Details
[X] Backend Service:
   - Dedicated metaplexNftService.js
   - Umi framework integration
   - Helper methods for NFT operations
   - Express route integration

[X] Frontend Components:
   - NFT Node visual interface
   - Input fields for NFT creation
   - Asset upload handling
   - Collection management UI

### Key Features

1. NFT Creation Process
[X] Asset Preparation:
   - Image/media upload
   - Buffer conversion
   - Generic file creation
   - Storage provider integration

2. Metadata Management
[X] On-chain Metadata:
   - Name and symbol
   - URI pointer
   - Update authority
   - Collection verification

[X] Off-chain Metadata:
   - JSON file structure
   - Image/media links
   - Traits and attributes
   - Extended properties

3. Collection Support
[X] Collection Features:
   - Collection NFT creation
   - NFT grouping
   - Verification system
   - Membership management

### Technical Implementation

1. Service Integration
[X] Dependencies:
   - @metaplex-foundation packages
   - Umi framework setup
   - Storage provider integration

2. Core Functions
[X] NFT Operations:
   - createNft()
   - updateNft()
   - verifyCollection()
   - uploadAssets()

3. Security Measures
[X] Key Management:
   - Keypair handling
   - Authority management
   - Update permissions
   - Collection verification

### Best Practices

1. Asset Management
[X] Storage:
   - Permanent storage solutions
   - IPFS/Arweave integration
   - Buffer handling
   - MIME type validation

2. Error Handling
[X] Validation:
   - Input validation
   - Transaction verification
   - Storage confirmation
   - Collection verification

3. Code Organization
[X] Structure:
   - Modular services
   - Clear separation of concerns
   - Consistent patterns
   - Reusable components

### Next Steps
[ ] Review frontend NFT node implementation
[ ] Analyze asset upload flow
[ ] Examine collection verification process
[ ] Document security measures

## Current Task: NFT Node Migration Plan

### Overview
Migrate current NFT implementation to use Metaplex Umi framework while preserving:
- Purple color scheme and UI aesthetics
- Creator wallet display (60%/40% split shown in screenshot)
- Current node system integration

### Analysis of Current System
[X] Visual Elements:
- Purple theme
- URI input field
- Symbol field (NFT)
- Royalties percentage (7%)
- Creator wallet displays with percentages
- Mint NFT action button

### Migration Steps

1. Backend Service Migration
[ ] Create new metaplexNftService.js:
```javascript
// src/services/metaplexNftService.js
import { createUmi } from "@metaplex-foundation/umi-bundle-defaults";
import { mplTokenMetadata } from "@metaplex-foundation/mpl-token-metadata";
import { irysUploader } from "@metaplex-foundation/umi-uploader-irys";

export class MetaplexNftService {
  constructor() {
    this.umi = null;
    this.initUmi();
  }

  async initUmi() {
    // Initialize with existing connection
    this.umi = createUmi(connection)
      .use(mplTokenMetadata())
      .use(irysUploader());
  }

  async createNft(params) {
    // Preserve existing creator structure
    const { uri, symbol, royalties, creators } = params;
    // Implementation using Umi
  }
}
```

2. Frontend Component Updates
[ ] Preserve NFTNode.jsx structure:
```javascript
// src/components/NFTNode.jsx
const NFTNode = ({ data, ...props }) => {
  // Keep existing state and UI
  const [creators, setCreators] = useState(data.creators || []);
  
  // Update only the service calls
  const handleMint = async () => {
    const nftService = new MetaplexNftService();
    // Call new service while maintaining existing UI state
  };
  
  return (
    // Keep existing UI structure
    <div className="nft-node purple-theme">
      {/* Existing UI components */}
    </div>
  );
};
```

3. Data Model Alignment
[ ] Map current data model to Umi:
```typescript
interface NFTNodeData {
  uri: string;
  symbol: string;
  royalties: number;
  creators: {
    address: string;
    share: number;
  }[];
}

// Maps to Umi format:
interface UmiNFTData {
  uri: string;
  symbol: string;
  sellerFeeBasisPoints: number; // royalties * 100
  creators: {
    address: PublicKey;
    share: number;
    verified: boolean;
  }[];
}
```

4. Integration Points
[ ] Update service integration:
- Replace direct Solana calls with Umi
- Maintain existing event system
- Preserve node connection logic

5. Testing Strategy
[ ] Test cases:
- Visual regression tests
- Creator wallet functionality
- Metadata verification
- Collection integration

### Phased Rollout

1. Phase 1: Backend Migration
[ ] Implement MetaplexNftService
[ ] Add Umi initialization
[ ] Test service in isolation

2. Phase 2: Frontend Integration
[ ] Update NFTNode component
[ ] Maintain UI elements
[ ] Test visual consistency

3. Phase 3: Data Migration
[ ] Convert existing NFT data
[ ] Verify metadata structure
[ ] Test creator splits

4. Phase 4: Validation
[ ] End-to-end testing
[ ] Performance verification
[ ] UI regression testing

### Risk Mitigation

1. UI Consistency
- Maintain color scheme
- Preserve component structure
- Keep creator display format

2. Data Integrity
- Validate metadata conversion
- Verify creator shares
- Test royalty calculations

3. Performance
- Monitor transaction times
- Check memory usage
- Verify node responsiveness

### Success Metrics
[ ] Visual consistency maintained
[ ] Creator functionality preserved
[ ] Improved NFT creation reliability
[ ] Successful metadata handling
[ ] Maintained node system integration

### Next Steps
1. [ ] Create MetaplexNftService class
2. [ ] Update NFTNode component
3. [ ] Test UI consistency
4. [ ] Validate creator functionality
5. [ ] Deploy in phases